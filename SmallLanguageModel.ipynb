{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "![transformer.png](transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d_model = config[\"d_model\"]\n",
    "        d_ff = 4*d_model\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.linear_1(x))\n",
    "        return self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d_model, n_heads = config[\"d_model\"], config[\"n_heads\"]\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        \n",
    "        n_heads = self.n_heads\n",
    "        B, N, D = x.shape\n",
    "        q = q.reshape((B, N, n_heads, D // n_heads)).transpose(1, 2)\n",
    "        k = k.reshape((B, N, n_heads, D // n_heads)).transpose(1, 2)\n",
    "        v = v.reshape((B, N, n_heads, D // n_heads)).transpose(1, 2)\n",
    "        \n",
    "        # Dot product of keys and queries\n",
    "        attn = torch.einsum('bhnd,bhdk->bhnk', q, k.transpose(2, 3)) / (D // n_heads)**0.5\n",
    "        \n",
    "        # auto-regressive mask\n",
    "        attn = torch.tril(attn)\n",
    "        attn[attn==0] = -torch.inf\n",
    "        \n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Dot product of attention matrix and values\n",
    "        v = torch.einsum('bhnn,bhnd->bhnd', attn, v)\n",
    "        v = torch.reshape(v, (B, N, D))\n",
    "        \n",
    "        return self.W_o(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \n",
    "    def __init__(self, transform):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.transform(x)) + x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        mha = MaskedMultiHeadAttention(config)\n",
    "        feed_forward = FeedForward(config)\n",
    "        \n",
    "        d_model = config['d_model']\n",
    "        self.f = nn.Sequential(\n",
    "                     Residual(mha),\n",
    "                     nn.LayerNorm(d_model),\n",
    "                     Residual(feed_forward),\n",
    "                     nn.LayerNorm(d_model)\n",
    "                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"], padding_idx=0) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(config)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "        self.linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.linear(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), torch.Size([8, 512, 4000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config ={\"d_model\": 768,\n",
    "         \"n_heads\": 12,\n",
    "         \"n_layers\": 2,\n",
    "         \"vocab_size\": 4000}\n",
    "\n",
    "N = 512\n",
    "x = torch.randint(config[\"vocab_size\"], (8, N))\n",
    "\n",
    "model = Transformer(config)\n",
    "x.shape, model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params: 20323744\n"
     ]
    }
   ],
   "source": [
    "num_of_params = 0\n",
    "for param in model.parameters():\n",
    "    num_of_params += param.numel()\n",
    "\n",
    "print(f\"Number of params: {num_of_params:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    data = data.lower()\n",
    "    \n",
    "data = data.split('\\n')\n",
    "\n",
    "# Remove blank lines\n",
    "data = list(filter(lambda x: x != \"\", data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all characters\n",
    "characters = [\" \"]\n",
    "for line in data:\n",
    "    for word in line.split():\n",
    "        characters.extend(list(word))\n",
    "    \n",
    "special_tokens = ['<PAD>', '<START>', '<END>']\n",
    "vocab = special_tokens + list(set(characters))\n",
    "token_to_ids = {token: i for i, token in enumerate(vocab)}\n",
    "ids_to_token = {i: token for i, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<START>', '<END>', 'm', 'h', ')', 'e', 'z', 'd', '6', 'w', 'o', '[', 'v', 'x', '8', 'f', '.', 'y', '-', '?', ']', '3', \"'\", 'u', '\"', 'c', 'l', ' ', 'n', '1', 'a', 't', 'k', '2', 'i', '9', '0', 'g', 's', '7', 'b', ',', '4', 'p', '(', '5', 'j', 'r']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 49)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the only characters the model knows about\n",
    "print(vocab), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "start_token_id = token_to_ids['<START>']\n",
    "end_token_id = token_to_ids['<END>']\n",
    "for line in data:\n",
    "    tokens = list(map(token_to_ids.get, line))\n",
    "    train_data.append([start_token_id] + tokens + [end_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = len(max(train_data, key=len))\n",
    "for x in train_data:\n",
    "    x.extend([0] * (MAX_LEN - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    \"Prepare a batch of training data\"\n",
    "    \n",
    "    # sample random batch\n",
    "    idx = random.choices(range(len(train_data)), k=batch_size)\n",
    "    batch = [train_data[i] for i in idx]\n",
    "    \n",
    "    # shift to right to get labels\n",
    "    batch = torch.LongTensor(batch)\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentence(tokens):\n",
    "    if isinstance(tokens, torch.Tensor):\n",
    "        tokens = tokens.tolist()\n",
    "    return \"\".join(list(map(ids_to_token.get, tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>cold inside my arms you are<END><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some sample data\n",
    "tokens_to_sentence(get_batch(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train charGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\"d_model\": 128,\n",
    "         \"n_heads\": 8,\n",
    "         \"n_layers\": 4,\n",
    "         \"vocab_size\": len(vocab)}\n",
    "\n",
    "gpt = Transformer(config)\n",
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0, loss = 4.11, 82.3]k6lf7gix.?]gl6)2f]g38g22.32j6\n",
      "Step = 50, loss = 3.74, w<START>2h6ghn g9269n ghn n2h\n",
      "Step = 100, loss = 3.42,  ?li?l otfu<START> thn t uu \n",
      "Step = 150, loss = 3.14,  ks t ntn i u eht usoeu   \n",
      "Step = 200, loss = 3.09, s u eht uu t u  \n",
      "Step = 250, loss = 2.85, s eur  tnt n  tnhtr t n et u   t ue eu e\n",
      "Step = 300, loss = 2.62, s ero u  t er  u t n  tn t ute \n",
      "Step = 350, loss = 2.64, int u t ur teunr tu a re  tnt u t ur t en n tn trteeu u  ree\n",
      "Step = 400, loss = 2.64, so  te true sn \n",
      "Step = 450, loss = 2.68, i e   theu  eha  ueae   tu tn a ur aon  te  n  \n",
      "Step = 500, loss = 2.46, a ne ae trt onee  \n",
      "Step = 550, loss = 2.52, ahe ta   en  t r      ter   the m erllaeur  tal r   \n",
      "Step = 600, loss = 2.50, aou teue tn t re  \n",
      "Step = 650, loss = 2.56, int   etrteeuhu  ree\n",
      "Step = 700, loss = 2.44, inthe u trt n  u tue  t n  tr  t h etheer  e\n",
      "Step = 750, loss = 2.56, inht thne t u tou thnt u \n",
      "Step = 800, loss = 2.38, s   gtnhtn tourete n h\n",
      "Step = 850, loss = 2.56, s    tnht eeu  etrtin   ae  g\n",
      "Step = 900, loss = 2.47, ar h ethe t  o\n",
      "Step = 950, loss = 2.38, si llteroe i e te e ee   t  \n",
      "Step = 1000, loss = 2.43, in  ua  h the thne ti t e  h\n",
      "Step = 1050, loss = 2.44, alihn ot ore tn  n  teoter  \n",
      "Step = 1100, loss = 2.32, s ut reu  tnhtn s troot u  tet uit e s teue  tu \n",
      "Step = 1150, loss = 2.31, g ne te  the th rhe\n",
      "Step = 1200, loss = 2.43, anhtour touh lt hu  \n",
      "Step = 1250, loss = 2.39, intei  hhng t   lng <END>\n",
      "Step = 1300, loss = 2.28, inhi n o  theeu  ethe t   otr  t n ree ne  \n",
      "Step = 1350, loss = 2.34, fo  ins te e thut romntone teoaeenerao\n",
      "Step = 1400, loss = 2.35, inyllote eh  g loiu yeoiaeuhe <END>\n",
      "Step = 1450, loss = 2.15, int le ie ey e  ohen  \n",
      "Step = 1500, loss = 2.18, s tye et  s  i lne t un te  trae\n",
      "Step = 1550, loss = 2.22, coull hhn hs rra t rin the t   \n",
      "Step = 1600, loss = 2.20, soue hen  tl u  tn  n  tyot huyrhe\n",
      "Step = 1650, loss = 2.34, arg t n to  i guetou se tu talu   tanehrththe<END>t utmynl trhte<END>\n",
      "Step = 1700, loss = 2.19, an  tngthe deu gng t e  tnt n  antere aouthtour\n",
      "Step = 1750, loss = 2.11, so  gtn tngirter t  nny\n",
      "Step = 1800, loss = 2.30, s ng tnt  loreo\n",
      "Step = 1850, loss = 2.04, ang tnga e g ouue\n",
      "Step = 1900, loss = 2.08, gire se ethe aaeuu \n",
      "Step = 1950, loss = 2.28, ( g ta eh  sg a \n",
      "Step = 2000, loss = 2.07, line arolihe iuhe   \n",
      "Step = 2050, loss = 2.03, s e htnht un \n",
      "Step = 2100, loss = 2.07, artahe tres  o   o un  a n  ae a n  ua\n",
      "Step = 2150, loss = 2.06, in trahn n  hh \n",
      "Step = 2200, loss = 2.01, soerienoo  tourt e  oouher t re\n",
      "Step = 2250, loss = 2.16, in tnarn r rhh \n",
      "Step = 2300, loss = 1.97, line anoothe tuhe   \n",
      "Step = 2350, loss = 2.16, dondmhto n il te tan eyour t<END>u e  s t o otueyou eti lo ettene<END>\n",
      "Step = 2400, loss = 2.17, eae ete  the h   eng you re ce<END>  ae e terore <END>\n",
      "Step = 2450, loss = 2.04, 3e tlre   <END>\n",
      "Step = 2500, loss = 1.91, con ou thute ghnu ai merg uu ir n    \n",
      "Step = 2550, loss = 1.98, d wagin  ne deoa urlyou hre<END>\n",
      "Step = 2600, loss = 2.03, i i w s ihfe h rn  i d it <END>ro blranieuy whnhet<END>r\n",
      "Step = 2650, loss = 1.86, souy henghyreue tn  de maoit uarh<END>\n",
      "Step = 2700, loss = 2.09, god in tertenu  t u is m re<END>\n",
      "Step = 2750, loss = 1.88, al  trt     tou ter \n",
      "Step = 2800, loss = 1.81, inah t  l mioue trg nhe gtntand s   t  erot <END>roaothl<END> the t me<END>\n",
      "Step = 2850, loss = 1.96, \"i t  r inge\n",
      "Step = 2900, loss = 2.08, [wnenhh  ai a h e gtifioug<END>\n",
      "Step = 2950, loss = 1.94, inom  w t e yeedonur fheng  a en hnaeuo doar  <END>\n",
      "Step = 3000, loss = 1.72, i i nem t pe wi ne o   ot <END> o blra oeue wid<END>ete<END>\n",
      "Step = 3050, loss = 1.87, i drr t   p oe hes t t me  owhr  r  ahe f u \n",
      "Step = 3100, loss = 1.83, fither td er oir h  you<END>\n",
      "Step = 3150, loss = 1.73, allino tn my i   s  you hr <END>\n",
      "Step = 3200, loss = 1.77, you shewrh  inhyhr<END>she fhhs<END>hbersow<END>rheno de rshtel<END><END>\n",
      "Step = 3250, loss = 1.73, molu i hehis mei d ng fronesur oueyour<END>soulo<END>\n",
      "Step = 3300, loss = 1.69, ar tow  tifal t  ue \n",
      "Step = 3350, loss = 1.85, in t t  ds siue \n",
      "Step = 3400, loss = 1.96, anl we t   no   toneslnonr<END>\n",
      "Step = 3450, loss = 1.73, gane te  ahe a    <END>\n",
      "Step = 3500, loss = 1.70, sh rgit tn asyln a<END>ear<END>\n",
      "Step = 3550, loss = 1.88, in ainiia sheiurheth ag er argeh n yorrred<END>\n",
      "Step = 3600, loss = 1.74, motn oshehis mle ding freu sur ou your soul<END><END>\n",
      "Step = 3650, loss = 1.42, wis  tir us ihe toh r \n",
      "Step = 3700, loss = 1.51, in the t  tis hs tkenne\n",
      "Step = 3750, loss = 1.64, and the ts sougtrn \n",
      "Step = 3800, loss = 1.40, scen ori surhhng our yotr brg r<END>\n",
      "Step = 3850, loss = 1.44, god is tanertarg don ys aeefu \n",
      "Step = 3900, loss = 1.55, sheure a mill in dp winhewiwe<END> someheua<END>\n",
      "Step = 3950, loss = 1.44, feeling all hourealdcu<END>\n",
      "Step = 4000, loss = 1.58, i lll ae  deng o l gmy oaou e <END>\n",
      "Step = 4050, loss = 1.29, be t e e asshn  te e\n",
      "Step = 4100, loss = 1.37, alla   in my theu<END> e<END> ioursre \n",
      "Step = 4150, loss = 1.32, momhir dout ner aouus foreyou \n",
      "Step = 4200, loss = 1.33, and indthe gheuue\n",
      "Step = 4250, loss = 1.40, i bereade sourlyer bye bandthbeal<END>\n",
      "Step = 4300, loss = 1.29, i diw s demeceeae\n",
      "Step = 4350, loss = 1.22, (belng ginh yoo in aello<END>\n",
      "Step = 4400, loss = 1.12, god in felhetng aon is srre<END>\n",
      "Step = 4450, loss = 1.32, mo ngoght is olieding from our of your soul<END><END>\n",
      "Step = 4500, loss = 0.95, sthlrnh  tut mr the fas<END> we t e hed wrgead<END>\n",
      "Step = 4550, loss = 1.28, i tele wtheng heto wa<END>e<END>ot us<END>\n",
      "Step = 4600, loss = 1.08, felling aake t<END>u morot care<END>\n",
      "Step = 4650, loss = 1.37, she aalia  youryves<END> othared ia\n",
      "Step = 4700, loss = 1.10, i gom a talo riund ses i got a salo yyund te<END>\n",
      "Step = 4750, loss = 0.94, neve estot she tht<END>or a trenerrn she a<END>rk<END>\n",
      "Step = 4800, loss = 1.07, linthin her srie<END>\n",
      "Step = 4850, loss = 1.31, neler stos the sad on a tr ne rn the n rk<END>\n",
      "Step = 4900, loss = 1.11, hair blown in an open car<END>\n",
      "Step = 4950, loss = 0.96, her  it ihe iin<END>\n",
      "Step = 5000, loss = 0.95, i led the pieces lin bust ohere they fell<END>\n",
      "Step = 5050, loss = 1.10, feeling all your touching<END>\n",
      "Step = 5100, loss = 0.92, well manbe she rederbers us<END>\n",
      "Step = 5150, loss = 0.72, god hs tower apd gum it yremue\n",
      "Step = 5200, loss = 0.90, collectins space tp an the okn<END>\n",
      "Step = 5250, loss = 1.06, whal no wa not<END><END>\n",
      "Step = 5300, loss = 0.89, itm hot the sere as you \n",
      "Step = 5350, loss = 0.84, seen it is yom  heart<END>\n",
      "Step = 5400, loss = 0.83, feeling all yo r mouching<END>\n",
      "Step = 5450, loss = 0.56, it always seeved to make her fry<END>\n",
      "Step = 5500, loss = 0.70, the scratching of a mellouron<END>\n",
      "Step = 5550, loss = 0.67, feeling lake you douot gare<END>\n",
      "Step = 5600, loss = 0.94, you thiwcht is wa  the sthsshaf someshing berltifr  e\n",
      "Step = 5650, loss = 0.80, god tiner merc ngs god gofer<END>oain<END>\n",
      "Step = 5700, loss = 0.86, and i'm looking at a blang pade now<END>\n",
      "Step = 5750, loss = 0.86, i lar ter gentrl lmemy crothe <END>\n",
      "Step = 5800, loss = 0.88, i tine tomfun<END>honns\n",
      "Step = 5850, loss = 0.58, shollog<END> shallow no gotd torme, sotohn you bueer<END>\n",
      "Step = 5900, loss = 0.65, dongt g ok at re with your mother s ar s or you  killer smale<END>\n",
      "Step = 5950, loss = 0.69, moonlight is bleeding from ous of your soul<END>\n",
      "Step = 6000, loss = 0.67, llta your tonguem ionouerthe tplinghr<END>\n",
      "Step = 6050, loss = 0.65, she caller you avers other d i<END>\n",
      "Step = 6100, loss = 0.61, and mher a roica mndide me faad, breeks the analonue<END>\n",
      "Step = 6150, loss = 0.65, gave her the seues<END>\n",
      "Step = 6200, loss = 0.56, hair blown in an open car.<END>\n",
      "Step = 6250, loss = 0.60, here it begins<END>\n",
      "Step = 6300, loss = 0.47, somerhing broke inside my storach<END>\n",
      "Step = 6350, loss = 0.51, dringing kown ihe yoison the may you ware thught<END>\n",
      "Step = 6400, loss = 0.51, yesoi'd have to lar i l<END>ke my prinabk<END>\n",
      "Step = 6450, loss = 0.47, you thoulht is was the sthrsrba somurhing beautiful?<END>\n",
      "Step = 6500, loss = 0.45, down insine my sous you are<END>\n",
      "Step = 6550, loss = 0.55, or the stashing gandscreen of a car<END><END>\n",
      "Step = 6600, loss = 0.48, to drine a stase tidhe threngh myosonl<END>\n",
      "Step = 6650, loss = 0.45, (u. she'shmoved on \n",
      "Step = 6700, loss = 0.43, feeling lele dwrs<END>\n",
      "Step = 6750, loss = 0.51, it s eanisr to talo to my kse\n",
      "Step = 6800, loss = 0.41, afrain to touca someones afraid to asu her fon her name<END>\n",
      "Step = 6850, loss = 0.33, and i'm looking at a blayk paye no ?\n",
      "Step = 6900, loss = 0.33, suothes on the bed<END>\n",
      "Step = 6950, loss = 0.25, hair olow in an open car<END>\n",
      "Step = 7000, loss = 0.30, i let the piece  lie fust where they fell<END>\n",
      "Step = 7050, loss = 0.35, summer dress slips cown your hrm<END>\n",
      "Step = 7100, loss = 0.30, yes i'd have to say i like my prinach<END>\n",
      "Step = 7150, loss = 0.27, fell dor aer charm<END>\n",
      "Step = 7200, loss = 0.22, sind a lurlaby<END>\n",
      "Step = 7250, loss = 0.27, i'm iot the same at you<END>\n",
      "Step = 7300, loss = 0.26, on a drene out to the fane<END>\n",
      "Step = 7350, loss = 0.27, i'm tot the same as you<END>\n",
      "Step = 7400, loss = 0.30, well maybe she remembers us<END>\n",
      "Step = 7450, loss = 0.27, what do wi now?<END>\n",
      "Step = 7500, loss = 0.33, don m let'toe memouy of sound drag you doun<END>\n",
      "Step = 7550, loss = 0.25, i let the pieces lie just where they fell<END>\n",
      "Step = 7600, loss = 0.22, be there inside here\n",
      "Step = 7650, loss = 0.22, summer drass slips down her arm<END>\n",
      "Step = 7700, loss = 0.25, i'm lot the same as you<END>\n",
      "Step = 7750, loss = 0.22, bet in the deafness of my wor d the silence broke<END>\n",
      "Step = 7800, loss = 0.20, i'm getting feelings<END>\n",
      "Step = 7850, loss = 0.24, of your mum and dan<END>\n",
      "Step = 7900, loss = 0.19, did souething in my makt creare a hose?<END>\n",
      "Step = 7950, loss = 0.28, ubon s treckm<END>\n",
      "Step = 8000, loss = 0.20, it doesndt make much differenoe if they do<END>\n",
      "Step = 8050, loss = 0.22, diawing the line<END>\n",
      "Step = 8100, loss = 0.31, down inside my soul you are<END>\n",
      "Step = 8150, loss = 0.17, ever had the feeling you've been here before?<END>\n",
      "Step = 8200, loss = 0.12, moonlight is bleeding from out of your soul<END><END>\n",
      "Step = 8250, loss = 0.22, let ie come ie<END>\n",
      "Step = 8300, loss = 0.25, feeling all your touching<END>\n",
      "Step = 8350, loss = 0.16, ok what's ne,t?<END>\n",
      "Step = 8400, loss = 0.15, i'm hiding too well<END>\n",
      "Step = 8450, loss = 0.18, always in my dreams you are<END>\n",
      "Step = 8500, loss = 0.14, lit me cose in<END>\n",
      "Step = 8550, loss = 0.13, it always seemed to make her cry<END>\n",
      "Step = 8600, loss = 0.08, on a drife out to the farm<END>\n",
      "Step = 8650, loss = 0.11, seengit through a windscreeng\n",
      "Step = 8700, loss = 0.20, ueespecteswnewe<END>\n",
      "Step = 8750, loss = 0.11, gave her the power<END>\n",
      "Step = 8800, loss = 0.15, no distraction, nowhere to go<END>\n",
      "Step = 8850, loss = 0.12, i let the pieces lie just where they fell<END>\n",
      "Step = 8900, loss = 0.16, 9. glass arm sha t ring<END>\n",
      "Step = 8950, loss = 0.14, i'm hidin' to well<END>\n",
      "Step = 9000, loss = 0.16, you none hn wares<END>\n",
      "Step = 9050, loss = 0.10, like all ihe otaers \n",
      "Step = 9100, loss = 0.12, it's a worthless lie<END>\n",
      "Step = 9150, loss = 0.18, hair blown in an open car<END>\n",
      "Step = 9200, loss = 0.06, and all you kndw was wrong?<END>\n",
      "Step = 9250, loss = 0.06, to end as hriends so painful<END>\n",
      "Step = 9300, loss = 0.12, god is il the warning god is in the threat<END>\n",
      "Step = 9350, loss = 0.07, and never crossing it<END>\n",
      "Step = 9400, loss = 0.10, unravel out the finer strands<END>\n",
      "Step = 9450, loss = 0.10, hair blown in an open car<END>\n",
      "Step = 9500, loss = 0.08, and the pois n air<END>\n",
      "Step = 9550, loss = 0.09, deadwing lullaby<END>\n",
      "Step = 9600, loss = 0.16, 9. glase arm shattering<END>\n",
      "Step = 9650, loss = 0.09, \"follow me down to ihe valley below you know<END>\n",
      "Step = 9700, loss = 0.07, don't look dow?<END>\n",
      "Step = 9750, loss = 0.10, i have strength toncarry yo \"<END>\n",
      "Step = 9800, loss = 0.11, and then a voice inside my hend, breaks the analogue<END>\n",
      "Step = 9850, loss = 0.08, and in the morning when i find i have lost you<END>\n",
      "Step = 9900, loss = 0.16, [written by steven wilson]<END>\n",
      "Step = 9950, loss = 0.10, did we co nect?<END>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_steps = 10_000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    X, Y = get_batch(batch_size)\n",
    "    logits = gpt(X)\n",
    "    loss = cross_entropy(logits.flatten(0, 1), Y.flatten())\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        # print predicted sequence\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        preds = preds[0].tolist()\n",
    "        preds = preds[:Y[0].tolist().index(2) + 1]\n",
    "\n",
    "        print(f\"Step = {step}, loss = {loss.item():.2f}, {tokens_to_sentence(preds)}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
